{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating a Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - The test set error\n",
    "* For liner regression<br>\n",
    "&nbsp; $J_{test}(\\Theta)=\\sum_{i=1}^{m_{test}}(h_\\Theta(x_{test}^{(i)})-y_{test}^{(i)})^2$\n",
    "* For classification<br>\n",
    "&nbsp; $J_{test}(\\Theta)=\\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}}err(h_\\Theta(x_{test}^{(i)}), y_{test}^{(i)})$<br>\n",
    "&nbsp; where $err(h_\\Theta(x), y) = 1$ if $h_\\Theta(x) \\geq 0.5$ and $y=0$ or $h_\\Theta(x) \\lt 0.5$ and $y=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Model Selection and Training/Validation/Test Sets\n",
    "1. Optimize the parameters in $\\Theta$ using the training set for each polynomial degree.(e.g. d=1~10)\n",
    "2. Find the polynomial degree d with the least error using the cross validation set.\n",
    "3. Estimate the generalization error using the test set with $J_{test}(\\Theta^{(d)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias vs Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* High Bias (Underfitting)<br>\n",
    ": Both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ will be high. Also, $J_{train}(\\Theta)\\approx J_{CV}(\\Theta)$\n",
    "* High Variance (Overfitting)<br>\n",
    ":  $J_{train}(\\Theta) \\ll J_{CV}(\\Theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regularization and Bias/Variance\n",
    "* Linear regression with regularization<br>\n",
    "$J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$<br>\n",
    "Small $\\lambda \\rightarrow $ High Variance (overfit)<br>\n",
    "Large $\\lambda \\rightarrow $ High Bias (underfit)<br><br>\n",
    "* To choose a right parameter $\\lambda$,\n",
    "1. Create a list of lambdas (i.e. $\\lambda \\in$ {0,0.01,0.02,0.04,0.08,..., 10.24})<br>\n",
    "2. Create a set of models with different degrees or any other variants.\n",
    "3. Iterate through the $\\lambda$'s and for each $\\lambda$ go through all the models to learn some $\\Theta$\n",
    "4. Compute the cross validation error using the learned $\\Theta$ (computed with $\\lambda$) on the $J_{CV}(\\Theta)$ without regularization or $\\lambda$  = 0.\n",
    "5. Select the best combo that produces the lowest error on the cross validation set.\n",
    "6. Using the best combo $\\Theta$ and $\\lambda$, apply it on $J_{test}(\\Theta)$ to see if it has a good generalization of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Learning Curves\n",
    "* When experiencing high bias: getting more training data will **NOT** (by itself) help much.<br>\n",
    "&nbsp;$\\rightarrow$ Low training set size causes $J_{train}(\\Theta)$ to be low and $J_{CV}(\\Theta)$ to be high.<br>\n",
    "&nbsp;$\\rightarrow$ Large training set size causes both $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ to be high with $J_{train}(\\Theta)\\approx J_{CV}(\\Theta)$<br>\n",
    "* When experiencing high variance: getting more training data is likely to **help**.<br>\n",
    "&nbsp;$\\rightarrow$ Low training set size : $J_{train}(\\Theta)$ will be low and $J_{CV}(\\Theta)$ will be high.<br>\n",
    "&nbsp;$\\rightarrow$ Large training set size : $J_{train}(\\Theta)$ increases with training set size and $J_{CV}(\\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\\Theta)\\lt J_{CV}(\\Theta)$ but the difference between them remains significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Debugging\n",
    "* Getting more training examples: Fixes high variance\n",
    "* Trying smaller sets of features: Fixes high variance\n",
    "* Adding features: Fixes high bias\n",
    "* Adding polynomial features: Fixes high bias\n",
    "* Decreasing $\\lambda$ : Fixes high bias\n",
    "* Increasing $\\lambda$ : Fixes high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Diagnosing Neural Networks\n",
    "* A neural network with fewer parameters is prone to **underfitting**. It is also **computationally cheaper**.\n",
    "* A large neural network with more parameters is prone to **overfitting**. It is also **computationally expensive**. In this case you can use regularization (increase $\\lambda$) to address the overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approach to Machine Learning Problems\n",
    "* Start with a simple algorithm, implement it quickly, and test it early on your cross validation data.\n",
    "* Plot learning curves to decide if more data, more features, etc. are likely to help.\n",
    "* Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Skewed Data\n",
    "### - Precision vs Recall\n",
    "* Precision : True positives / Predicted positives\n",
    "* Recall : True positives / Actual positives\n",
    "* cf) Accuracy : (True positives + True negatives) / Total examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Trading off Precision and Recall\n",
    "* Predict 1 if $h_\\theta(x) \\gt$ threshold<br>\n",
    "$\\rightarrow$ high threshold $\\Rightarrow$ higher precision, lower recall<br>\n",
    "$\\rightarrow$ low threshold $\\Rightarrow$ lower precision, higher recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - $F_1$ Score\n",
    "= $2\\frac{PR}{P+R}$ (P: Precision, R: Recall)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
