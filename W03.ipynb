{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification and Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Classification\n",
    "* Classification is **Logistic Regression**<br>\n",
    ": Classification is NOT Linear Regression<br>\n",
    "\n",
    "### &nbsp; cf) Linear Regression vs Logistic Regression<br>\n",
    "* **Similarities**<br>\n",
    "&nbsp; - Both are supervised Machine Learning algorithms.<br>\n",
    "&nbsp; - Both are parametric regression i.e. both the models use linear equations for predictions. <br>\n",
    "\n",
    "* **Differences**<br>\n",
    "&nbsp; - Linear Regression is used to handle *regression* problems whereas Logistic regression is used to handle the *classification* problems.<br>\n",
    "&nbsp; - Linear regression provides a *continuous output* but Logistic regression provides *discreet output*.<br>\n",
    "&nbsp; - The purpose of Linear Regression is to find the *best-fitted line* while Logistic regression is one step ahead and fitting the line values to the sigmoid curve.<br>\n",
    "&nbsp; - The method for calculating loss function in linear regression is the *mean squared error* whereas for logistic regression it is *maximum likelihood estimation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Hypothesis Representation\n",
    "* Logistic Function : $h(x) = g(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}$ &nbsp; $(0\\le h(x) \\le 1)$<br>\n",
    "* $h(x)$ is the probability that $y=1$, given $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Decision Boundary (for discrete 0 or 1)\n",
    "* $h(x) \\ge 0.5 \\rightarrow y = 1$\n",
    "* $h(x) \\lt 0.5 \\rightarrow y = 0$\n",
    "* The boundary is when $h(x)=g(\\theta^Tx)=0.5, \\Leftrightarrow \\theta^Tx=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Cost Function<br>\n",
    "* We cannot use the same cost function that we use for linear regression.<br>\n",
    "* $J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}Cost(h_\\theta(x),y)$<br>\n",
    "&nbsp; $Cost(h(x),y) = -ylog(h_\\theta(x))-(1-y)log(1-h_\\theta(x))$ &nbsp; y=0 or 1<br>\n",
    "&nbsp; Therefore, $J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}y_i\\log(h_\\theta(x_i))+(1-y_i)\\log(1-h_\\theta(x_i))$<br>\n",
    "* This cost function guarantees that $J(\\theta)$ is convex for logistic regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Gradient Descent<br>\n",
    "* $\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)x_{ij}$ where $j$=0, 1, ..., n<br>\n",
    "* Notice that this algorithm is identical to the one we used in linear regression, but with different $h_\\theta(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Advanced Optimization<br>\n",
    "* Conjugate gradient<br>\n",
    "* BFGS<br>\n",
    "* L-BFGS<br>\n",
    "* Code in Octave/Matlab<br>\n",
    ": function [jVal, gradient] = costFunction(theta)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; jVal = [...code to compute J(theta)...];<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; gradient = [...code to compute derivative of J(theta)...];<br>\n",
    "&nbsp; end<br>\n",
    "&nbsp; options = optimset('GradObj', 'on', 'MaxIter', 100);<br>\n",
    "&nbsp; initialTheta = zeros(2,1);<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp; : We have more than two categories. Instead of y = {0,1}, we have y = {0,1...n}.<br>\n",
    "### - One vs All(Rest)<br>\n",
    "* Treat n+1 cases where in each case is a binary classification problem.<br>\n",
    "&nbsp; i.e y=0 $\\rightarrow$ 1, &nbsp; &nbsp; y=1, 2, ... , n $\\rightarrow$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; y=1 $\\rightarrow$ 1, &nbsp; &nbsp; y=0, 2, ... , n $\\rightarrow$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; y=n $\\rightarrow$ 1, &nbsp; &nbsp; y=0, 1, ... , n-1 $\\rightarrow$ 0<br>\n",
    "* Then we will get each of $h_\\theta^{(n)}(x)$ which is the possibility that x=n.<br>\n",
    "* With a new feature $x$, compute each of $h_\\theta^{(0)}(x), h_\\theta^{(1)}(x), ..., h_\\theta^{(n)}(x)$ and choose the highest probability.<br>\n",
    "&nbsp; &nbsp; $\\Rightarrow prediction = \\max_{i}(h_\\theta^{(i)}(x))$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solving the Problem of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Options to correct Overfitting<br>\n",
    "1) Reduce the number of features<br>\n",
    "&nbsp; - Manually select which features to keep.<br>\n",
    "&nbsp; - Use a model selection algorithm.<br>\n",
    "2) Regularization<br>\n",
    "&nbsp; - Keep all the features, but reduce the magnitude of parameters $\\theta_j$.<br>\n",
    "&nbsp; - Regularization works well when we have a lot of slightly useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Modifying Cost Function<br>\n",
    "* If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost.<br>\n",
    "* $\\min_{\\theta}J(\\theta) = \\min_{\\theta}\\frac{1}{2m} [\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)^2 + \\lambda\\sum_{j=1}^{n}\\theta_j^2]$ ($j$=1, ..., n)<br>\n",
    "* $\\lambda$ : regularization parameter<br>\n",
    "* If $\\lambda$ is chosen to be too large, it may smooth out the function too much and cause underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regularized Linear Regression<br>\n",
    "* Since the modified cost function doesn't include $\\theta_0$, we will modify our gradient descent function to separate out $\\theta_0$ from the rest of the parameters.<br>\n",
    "* Repeat{$$\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)x_{i0}$$\n",
    "$$\\theta_j := \\theta_j - \\alpha[(\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)x_{ij})+\\frac{\\lambda}{m}\\theta_j]$$\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; where ($j$=1, ..., n)\n",
    "}<br>\n",
    "* = $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)x_{ij}$ &nbsp; &nbsp; where ($j$=1, ..., n)<br>\n",
    "* **Normal Equation**<br>\n",
    "&nbsp; $\\theta = (X^TX + \\lambda L)^{-1}X^Ty$<br>\n",
    "&nbsp; where $L = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & ... & 0\\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}$<br>\n",
    "* L is a matrix with 0 at the top left and 1's down the diagonal, with 0's everywhere else. It should have dimension (n+1)Ã—(n+1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Regularized Logistic Regression<br>\n",
    "* Modified Cost Function<br>\n",
    "&nbsp; $J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}[y_i\\log(h_\\theta(x_i))+(1-y_i)\\log(1-h_\\theta(x_i))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$ &nbsp; &nbsp; ($j$=1, ..., n)<br>\n",
    "* Gradient descent (Same with Linear Regression)<br>\n",
    "&nbsp; Reapeat{$$\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)x_{i0}$$\n",
    "$$\\theta_j := \\theta_j - \\alpha[(\\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x_i)-y_i)x_{ij})+\\frac{\\lambda}{m}\\theta_j]$$\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; where ($j$=1, ..., n), $h(x) = \\frac{1}{1+e^{-\\theta^Tx}}$\n",
    "}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
