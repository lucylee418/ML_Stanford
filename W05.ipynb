{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cost Function and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Cost Function of Neural Networks\n",
    "* L = total number of layers in the network\n",
    "* $s_l$ = number of units (not counting bias unit) in layer $l$\n",
    "* K = number of output units/classes\n",
    "* $h_\\Theta(x)_k$ = hypothesis that results in the k-th output. (we may have many output nodes)\n",
    "* $J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[y_{ik}\\log(h_\\Theta(x_i)_k)+(1-y_ik)\\log(1-h_\\Theta(x_i)_k)]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{l,ji})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Backpropagation of Neural Networks\n",
    "* Goal: To find $\\min_{\\Theta}J(\\Theta)$<br>\n",
    "$\\Rightarrow$ Compute $\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta)$\n",
    "* Training set $\\{(x_1, y_1), ..., (x_m, y_m)\\}$<br>\n",
    "&nbsp;Set $\\Delta_{ij}^{(l)}=0$ for all $l, i, j$<br>\n",
    "&nbsp;For $i$=1 ~ $m$,<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Set $a^{(1)}=x^{(i)}$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Perform forward propagation to compute $a^{(l)}$(activation of unit in layer $l$) for $l=2, 3, ..., L$ (not 1)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Using $y^{(i)}$(actual value), compute $\\delta^{(L)}$(error)$=a^{(L)} - y^{(i)}$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ..., \\delta^{(2)}$ (not $\\delta^{(1)}$) using $\\delta^{(l)}=((\\Theta^{(l)})^T\\delta^{(l+1)})a^{(l)}(1-a^{(l)})$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $g'(z^{(l)})=a^{(l)}(1-a^{(l)})$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $\\Delta_{ij}^{(l)}:=\\Delta_{ij}^{(l)}+a_j^{(l)}\\delta_i^{(l+1)}$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Update $\\Delta$ matix,<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; * $D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}+\\lambda\\Theta_{ij}^{(l)}$ if j $\\neq$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; * $D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}$ if j $=$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (matrix D is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative.)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $\\Rightarrow \\therefore \\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)}$\n",
    "* $\\delta_2^{(2)} = \\Theta_{12}^{(2)}\\delta_1^{(3)}+\\Theta_{22}^{(2)}\\delta_2^{(3)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"forward_propagation.png\" alt=\"title\" width=\"400\"/>\n",
    "<br>\n",
    "<img src=\"forward_propagation(2).png\" alt=\"title\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Unrolling Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Putting it Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Autonomous Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
