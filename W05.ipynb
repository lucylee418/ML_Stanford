{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cost Function and Backpropagation of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Cost Function\n",
    "* L = total number of layers in the network\n",
    "* $s_l$ = number of units (not counting bias unit) in layer $l$\n",
    "* K = number of output units/classes\n",
    "* $h_\\Theta(x)_k$ = hypothesis that results in the k-th output. (we may have many output nodes)\n",
    "* $J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[y_{ik}\\log(h_\\Theta(x_i)_k)+(1-y_ik)\\log(1-h_\\Theta(x_i)_k)]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{l,ji})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Backpropagation\n",
    "* Goal: To find $\\min_{\\Theta}J(\\Theta)$<br>\n",
    "$\\Rightarrow$ Compute $\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta)$\n",
    "* Training set $\\{(x_1, y_1), ..., (x_m, y_m)\\}$<br>\n",
    "&nbsp;Set $\\Delta_{ij}^{(l)}=0$ for all $l, i, j$<br>\n",
    "&nbsp;For $i$=1 ~ $m$,<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Set $a^{(1)}=x^{(i)}$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Perform forward propagation to compute $a^{(l)}$(activation of unit in layer $l$) for $l=2, 3, ..., L$ (not 1)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Using $y^{(i)}$(actual value), compute $\\delta^{(L)}$(error)$=a^{(L)} - y^{(i)}$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ..., \\delta^{(2)}$ (not $\\delta^{(1)}$) using $\\delta^{(l)}=((\\Theta^{(l)})^T\\delta^{(l+1)})a^{(l)}(1-a^{(l)})$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $g'(z^{(l)})=a^{(l)}(1-a^{(l)})$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $\\Delta_{ij}^{(l)}:=\\Delta_{ij}^{(l)}+\\delta_i^{(l+1)}(a_j^{(l)})^T$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Update $\\Delta$ matix,<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; * $D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}+\\lambda\\Theta_{ij}^{(l)}$ if j $\\neq$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; * $D_{ij}^{(l)}:=\\frac{1}{m}\\Delta_{ij}^{(l)}$ if j $=$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (matrix D is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative.)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $\\Rightarrow \\therefore \\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta) = D_{ij}^{(l)}$\n",
    "* $\\delta_2^{(2)} = \\Theta_{12}^{(2)}\\delta_1^{(3)}+\\Theta_{22}^{(2)}\\delta_2^{(3)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"forward_propagation.png\" alt=\"title\" width=\"400\"/>\n",
    "<br>\n",
    "<img src=\"forward_propagation(2).png\" alt=\"title\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.000800000000007\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = 0.01\n",
    "i = 2*((a+b)**4)+2\n",
    "j = 2*((a-b)**4)+2\n",
    "print((i-j)/(2*b))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
