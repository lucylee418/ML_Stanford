{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cost Function and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Cost Function of Neural Networks\n",
    "* L = total number of layers in the network\n",
    "* $s_l$ = number of units (not counting bias unit) in layer $l$\n",
    "* K = number of output units/classes\n",
    "* $h_\\Theta(x)_k$ = hypothesis that results in the k-th output. (we may have many output nodes)\n",
    "* $J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}[y_{ik}\\log(h_\\Theta(x_i)_k)+(1-y_ik)\\log(1-h_\\Theta(x_i)_k)]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\theta_{l,ji})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Backpropagation of Neural Networks\n",
    "* Goal: To find $\\min_{\\Theta}J(\\Theta)$<br>\n",
    "$\\Rightarrow$ Compute $\\frac{\\delta}{\\delta\\Theta_{l, ij}}J(\\Theta)$\n",
    "* Training set $\\{(x_1, y_1), ..., (x_m, y_m)\\}$<br>\n",
    "&nbsp;Set $\\Delta_{l, ij}=0$ for all $l, i, j$<br>\n",
    "&nbsp;For $i$=1 ~ $m$,<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Perform forward propagation to compute $a_l$(activation of unit in layer $l$) for $l=2, 3, ..., L$ (not 1)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Using $y_i$(actual value), compute $\\delta_L$(error)$=a_L - y_i$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Compute $\\delta_{L-1}, \\delta_{L-2}, ..., \\delta_2$ (not $\\delta_1$) using $\\delta_l=(\\Theta_l^T\\delta_{l+1})a_l(1-a_l)$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $g'(z_l)=a_l(1-a_l)$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; $\\Delta_{l, ij}:=\\Delta_{l, ij}+a_{lj}\\delta_{(l+1), i}$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; Update $\\Delta$ matix,<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; * $D_{l, ij}:=\\frac{1}{m}(\\Delta_{l, ij}+\\lambda\\Theta_{l, ij})$ if j $\\neq$ 0<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; * $D_{l, ij}:=\\frac{1}{m}\\Delta_{l, ij}$ if $j=0$<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; (matrix D is used as an \"accumulator\" to add up our values as we go along and eventually compute our partial derivative.)<br>\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; $\\Rightarrow \\therefore \\frac{\\delta}{\\delta\\Theta_{l, ij}}J(\\Theta) = D_{l, ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Unrolling Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Putting it Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Autonomous Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
