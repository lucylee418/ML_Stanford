{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Multiple Features\n",
    "&nbsp; $h(x) = \\theta^Tx$<br>\n",
    "\n",
    "&nbsp; Note that $h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$&nbsp; (with n features). Define $x_0 = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Gradient Descent for Multiple Variables\n",
    "* New gradient descent algorithm<br>\n",
    "repeat : {\n",
    "    $$\\theta_j := \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h(x_i)-y_i)x_j$$\n",
    "    }<br>\n",
    "* Feature Scaling vs Mean Normalization<br>\n",
    "&nbsp; - Feature Scaling (dividing the input values by the range)\n",
    "$$x_i := \\frac{x_i}{max-min}$$\n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  - Mean Normalization  ($s_i$: range or std)<br>\n",
    "$$x_i := \\frac{x_i - \\mu_i}{s_i}$$ \n",
    "\n",
    "* Learning Rate ($\\alpha$)<br>\n",
    "&nbsp; - Debugging gradient descent<br>\n",
    " : Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.<br>\n",
    "&nbsp; - Automatic convergence test<br>\n",
    " : Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as $10^{-3}$. However in practice it's difficult to choose this threshold value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Features and Polynomial Regression\n",
    "* We can **combine** multiple features into one. i.e. $x_3 = x_1 * x_2$<br>\n",
    "* We can **change the behavior or curve**<br>\n",
    "&nbsp; - $h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_1^2$ (quadratic)<br>\n",
    "&nbsp; - $h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_1^2 + \\theta_3x_1^3$ (cubic)<br>\n",
    "&nbsp; - $h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2\\sqrt{x_1}$ (square root)<br>\n",
    "&nbsp; **$\\Rightarrow$ FEATURE SCALING is important**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computing Parameters Analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Normal Equation\n",
    "&nbsp; : Method to solve for $\\theta$ analytically (without iteration)<br>\n",
    "&nbsp; : Add feature $x_0 = 1$ to make matrix X<br>\n",
    "&nbsp; : $\\theta = (X^TX)^{-1}X^Ty$ &nbsp; (X : m*(n+1) matrix, y : m-dimensional vector)<br>\n",
    "&nbsp; : No need for feature scaling\n",
    "* Gradient Descent vs Nomal Equation<br>\n",
    "\n",
    "| Gradient Descent | Nomal Equation |\n",
    "| --- | --- |\n",
    "| Need to choose $\\alpha$ | No need to choose $\\alpha$ |\n",
    "| Iterations | No iteration |\n",
    "| Works well with large features | Slow with large features  |\n",
    "\n",
    "* Noninvertibility of $X^TX$<br>\n",
    "&nbsp; - Redundant features : multiple features are linearly dependent<br>\n",
    "&nbsp; - Too many features (e.g. m ≤ n) : delete some features or use *regularization*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MATLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Workflow\n",
    "1. Select X and y : X(features), y(target)<br>\n",
    "2. Plotting the data : to understand data better<br>\n",
    "3. Computing the cost $J(\\theta)$<br>\n",
    "&nbsp; - add a column of ones to X<br>\n",
    "&nbsp; - J = (sum((X * theta) - y))^2 / (2 * m);<br>\n",
    "4. Updating $\\theta$<br>\n",
    "5. Get the optimized $\\theta$<br>\n",
    "6. Plot the model\n",
    "7. Predict : using [1, $x_1$, ... , $x_n$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Syntax\n",
    "* eye(n); &nbsp; &nbsp; &nbsp; &nbsp;% n*n identical matrix\n",
    "* data = load('ex1data1.txt'); &nbsp; &nbsp; &nbsp; &nbsp; % read comma separated data\n",
    "* X = data(:, 1); y = data(:, 2); &nbsp; &nbsp; &nbsp; &nbsp; % (n-th row, n-th column)\n",
    "* plot(X, y, 'rx', 'MarkerSize', 10); &nbsp; &nbsp; &nbsp; &nbsp; % plot the data<br>\n",
    "xlabel('x label'); &nbsp; &nbsp; &nbsp; &nbsp; % set the x-axis label<br>\n",
    "ylabel('y label'); &nbsp; &nbsp; &nbsp; &nbsp; % set the y-axis label<br>\n",
    "* X = [ones(m,1),data(:,1)]; &nbsp; &nbsp; &nbsp; &nbsp; % Add a column of ones to x\n",
    "* for iter = 1:num_iters<br>\n",
    "&nbsp; theta = theta - alpha * (X' * (X * theta - y)) / m; &nbsp; &nbsp; &nbsp; &nbsp; % update theta<br> \n",
    "end<br>\n",
    "* fprintf('Theta computed from gradient descent:\\n%f,\\n%f',theta(1),theta(2)) &nbsp; &nbsp; &nbsp; &nbsp; % print $\\theta_0$ and $\\theta_1$<br>\n",
    "* hold on; &nbsp; &nbsp; &nbsp; &nbsp; % keep previous plot visible<br>\n",
    "&nbsp; plot(X(:,2), X*theta, '-') &nbsp; &nbsp; &nbsp; &nbsp; % plot $x_1$ and $y$<br>\n",
    "&nbsp; legend('Training data', 'Linear regression') &nbsp; &nbsp; &nbsp; &nbsp; % legend<br>\n",
    "&nbsp; hold off &nbsp; &nbsp; &nbsp; &nbsp; % don't overlay any more plots on this figure<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
